{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion MNIST is a dataset of ten categories of clothing and accessories, in grayscales. The purpose of the tutorial is to accurately assign each item into one of the ten categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 1s 26us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 37s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 7s 2us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer Perceptron\n",
    "Our image is 28x28, and therefore is two-dimensional. Because of our perceptron only able to read one-dimensional data, let’s flatten them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the hidden layer, let’s set an arbitrary number of neurons. The number should be simple and small enough to follow our step number 1. Let’s choose 10 neurons.\n",
    "- For the output layer, because we have 10 categories to categorize, we need to set it to 10 output neurons. For each image, each of these neurons will be filled with 1 if it is the correct category and 0 if not.\n",
    "- Always use categorical_crossentropy for multi-categories, and binary_crossentropy for two categories. Use adam or rmsprop as the optimizer since both of them are pretty good. And you need accuracy as the metric to check your network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=784, activation='relu')) # hidden layer\n",
    "model.add(Dense(10, activation='softmax')) # output layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 4s 82us/sample - loss: 0.6475 - accuracy: 0.7738 - val_loss: 0.4947 - val_accuracy: 0.8310\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 3s 63us/sample - loss: 0.4620 - accuracy: 0.8403 - val_loss: 0.4509 - val_accuracy: 0.8462\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 5s 93us/sample - loss: 0.4324 - accuracy: 0.8492 - val_loss: 0.4190 - val_accuracy: 0.8555\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 5s 84us/sample - loss: 0.4196 - accuracy: 0.8537 - val_loss: 0.4136 - val_accuracy: 0.8572\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 4s 73us/sample - loss: 0.4092 - accuracy: 0.8576 - val_loss: 0.4081 - val_accuracy: 0.8573\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 4s 75us/sample - loss: 0.4001 - accuracy: 0.8586 - val_loss: 0.4076 - val_accuracy: 0.8597\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 4s 68us/sample - loss: 0.3947 - accuracy: 0.8619 - val_loss: 0.4218 - val_accuracy: 0.8562\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 4s 74us/sample - loss: 0.3875 - accuracy: 0.8651 - val_loss: 0.4129 - val_accuracy: 0.8535\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 4s 70us/sample - loss: 0.3858 - accuracy: 0.8661 - val_loss: 0.4236 - val_accuracy: 0.8490\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 4s 66us/sample - loss: 0.3824 - accuracy: 0.8664 - val_loss: 0.4163 - val_accuracy: 0.8515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x144f4b908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(x_train, y_train, \n",
    "          epochs=10, # epochs is the number of training loops \n",
    "          validation_split=0.1) # use 10% of the training data as the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good. We get 85% accuracy on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.4465 - accuracy: 0.8449\n",
      "0.8449\n"
     ]
    }
   ],
   "source": [
    "# Testing performance\n",
    "_, test_acc = model.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 84% accuracy on test data means the network guessed right for around 8400 images from the 10K test data.\n",
    "- A higher accuracy on test data means a better network. If you think the accuracy should be higher, maybe you need the next step(s) in building your Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wider network: more hidden layer cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the number of the hidden layer cells. We’ve increased these from 10 to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 5s 90us/sample - loss: 0.5436 - accuracy: 0.8124 - val_loss: 0.4355 - val_accuracy: 0.8457\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 4s 73us/sample - loss: 0.4107 - accuracy: 0.8549 - val_loss: 0.3940 - val_accuracy: 0.8575\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 5s 95us/sample - loss: 0.3698 - accuracy: 0.8673 - val_loss: 0.3823 - val_accuracy: 0.8605\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 4s 78us/sample - loss: 0.3454 - accuracy: 0.8757 - val_loss: 0.3544 - val_accuracy: 0.8723\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 4s 76us/sample - loss: 0.3286 - accuracy: 0.8804 - val_loss: 0.3500 - val_accuracy: 0.8738\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 5s 87us/sample - loss: 0.3137 - accuracy: 0.8852 - val_loss: 0.3452 - val_accuracy: 0.8742\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 5s 89us/sample - loss: 0.3010 - accuracy: 0.8902 - val_loss: 0.3403 - val_accuracy: 0.8765\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 5s 91us/sample - loss: 0.2920 - accuracy: 0.8924 - val_loss: 0.3399 - val_accuracy: 0.8738\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 5s 85us/sample - loss: 0.2812 - accuracy: 0.8985 - val_loss: 0.3467 - val_accuracy: 0.8777\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 5s 92us/sample - loss: 0.2707 - accuracy: 0.9008 - val_loss: 0.3398 - val_accuracy: 0.8795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x145839438>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(50, input_dim=784, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model2.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A whopping 88% accuracy on validation data. Good! It proves that making a bigger network can increase the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.3580 - accuracy: 0.8732\n",
      "0.8732\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = model2.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper network: more hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 6s 119us/sample - loss: 0.5283 - accuracy: 0.8134 - val_loss: 0.4078 - val_accuracy: 0.8540\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 5s 99us/sample - loss: 0.3911 - accuracy: 0.8596 - val_loss: 0.3981 - val_accuracy: 0.8550\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.3556 - accuracy: 0.8689 - val_loss: 0.3770 - val_accuracy: 0.8643\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 5s 90us/sample - loss: 0.3287 - accuracy: 0.8797 - val_loss: 0.3592 - val_accuracy: 0.8677\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 6s 117us/sample - loss: 0.3139 - accuracy: 0.8849 - val_loss: 0.3393 - val_accuracy: 0.8737\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 6s 119us/sample - loss: 0.2980 - accuracy: 0.8895 - val_loss: 0.3694 - val_accuracy: 0.8657\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 6s 112us/sample - loss: 0.2881 - accuracy: 0.8934 - val_loss: 0.3335 - val_accuracy: 0.8790\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 6s 112us/sample - loss: 0.2786 - accuracy: 0.8965 - val_loss: 0.3387 - val_accuracy: 0.8772\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 5s 93us/sample - loss: 0.2696 - accuracy: 0.9008 - val_loss: 0.3349 - val_accuracy: 0.8770\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.2612 - accuracy: 0.9026 - val_loss: 0.3252 - val_accuracy: 0.8852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1460f4f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add one more hidden layer with 50 cells\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(50, input_dim=784, activation='relu'))\n",
    "model3.add(Dense(50, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model3.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.3554 - accuracy: 0.8773\n",
      "0.8773\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = model3.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement is not so big, maybe our approach is not right by using perceptron on images. How about we change network models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network (CNN) is a neural network that can “see” a subset of our data. It can detect a pattern in images better than perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train[:,:,:,np.newaxis] / 255.0\n",
    "x_test = x_test[:,:,:,np.newaxis] / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape # 60,000 x 28 x 28 x 1 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data CNN needs to read must be like this: total_data x width x height x channels.\n",
    "- Height and width are self-explanatory. Channels are like Red or Green or Blue in RGB images. In RGB, because there are 3 channels, we need to make the data x 3. But because we work with grayscale images, every value on Red, Green, or Blue channel is the same and we reduce to one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Conv2D(filters=64, kernel_size=2, \n",
    "                  padding='same', activation='relu', \n",
    "                  input_shape=(28,28, 1))) \n",
    "\n",
    "model4.add(MaxPooling2D(pool_size=2))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 64)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                125450    \n",
      "=================================================================\n",
      "Total params: 125,770\n",
      "Trainable params: 125,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv2d changes your 28x28x1 image to 28x28x64. Just imagine this as 64 hidden layer cells.\n",
    "- MaxPooling2D reduces the width and height so that you will not need to compute all the cells. It reduces the size to 14x14x64.\n",
    "- Flatten just flattens out the output of MaxPooling into a hidden layer of 12544 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 39s 722us/sample - loss: 0.4331 - accuracy: 0.8496 - val_loss: 0.3475 - val_accuracy: 0.8740\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 36s 669us/sample - loss: 0.3121 - accuracy: 0.8906 - val_loss: 0.3280 - val_accuracy: 0.8802\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 46s 854us/sample - loss: 0.2809 - accuracy: 0.9004 - val_loss: 0.2939 - val_accuracy: 0.8968\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 44s 824us/sample - loss: 0.2586 - accuracy: 0.9077 - val_loss: 0.2915 - val_accuracy: 0.8960\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 34s 629us/sample - loss: 0.2417 - accuracy: 0.9144 - val_loss: 0.2890 - val_accuracy: 0.8980\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 38s 695us/sample - loss: 0.2281 - accuracy: 0.9184 - val_loss: 0.2733 - val_accuracy: 0.9048\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 43s 793us/sample - loss: 0.2138 - accuracy: 0.9247 - val_loss: 0.2810 - val_accuracy: 0.8987\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 33s 620us/sample - loss: 0.2044 - accuracy: 0.9268 - val_loss: 0.2812 - val_accuracy: 0.8975\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 37s 688us/sample - loss: 0.1946 - accuracy: 0.9301 - val_loss: 0.2794 - val_accuracy: 0.9028\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 40s 735us/sample - loss: 0.1855 - accuracy: 0.9332 - val_loss: 0.2741 - val_accuracy: 0.9068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14725b390>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 202us/sample - loss: 0.2846 - accuracy: 0.9019\n",
      "0.9019\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = model4.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
